\chapter{Literature Review and Related Work}
\label{chap:relatedworks}

\section{Competitor Analysis}
\label{section:competitor-analysis}

\begin{table}[htbp!]
   \begin{adjustwidth}{-.85in}{-.85in}
       \noindent
       \centering
       \small\begin{tabularx}{1.3\textwidth}{|X|>{\columncolor{green!20}}X|X|X|X|}
           \hline & \textbf{\usevar{\srsTitle}} & \textbf{Hikvision} & \textbf{YOLORe-IDNet} & \textbf{JARVIS-MoCap} \\\hline
           \textbf{Primary Focus} & Campus tracking with persistent identity & Single-camera surveillance with minimal cross-camera capability & Real-time detection without integrated systems & Research motion capture in controlled settings \\\hline
           \textbf{Core Technology} & Multi-model fusion with spatial mapping & Proprietary algorithms optimized for single views & YOLO detection with separate Re-ID network & Markerless pose estimation with synced cameras \\\hline
           \textbf{Multi-Camera Detection} & Independent detection robust to occlusions & Strong within-view detection, poor across views & No multi-camera management system & Requires calibrated camera arrays \\\hline
           \textbf{Tracking and Re-ID} & Identity across non-overlapping views & Excellent within single views, fails across views & Matches appearance without trajectories & Tracks pose, not identity \\\hline
           \textbf{Spatial Representation} & Unified coordinate system visualization & Limited to individual camera views & No spatial mapping functionality & Limited to pre-calibrated environments \\\hline
           \textbf{System Accessibility} & Web interface with easy deployment & Requires dedicated hardware & Command-line interface for experts & Research software with complex setup \\\hline
           \textbf{Cross-Platform Support} & Any device with browser access & Specific hardware and OS requirements & Platform-dependent implementation & Desktop with specific dependencies \\\hline
       \end{tabularx}
   \end{adjustwidth}
   \caption{Competitor Analysis of \usevar{\srsTitle}}
\end{table}

In order to acknowledge the existing technology with similar functionalities, a competitor analysis is conducted.
There are various systems in the market that can track individuals, we have selected 3 of which most closely resemble our project:

\begin{itemize}
   \item \textbf{\textit{Hikvision AI Camera Systems}}---a commercial leader in the Thailand AI camera market offering excellent single-camera tracking but limited cross-camera capabilities.
   \item \textbf{\textit{YOLORe-IDNet}}---a research implementation combining YOLO object detection with re-identification networks for real-time tracking without prior knowledge.
   \item \textbf{\textit{JARVIS-MoCap}}---an open-source markerless 3D motion capture system designed for research environments requiring precise tracking.
\end{itemize}
\par
The \usevar{\srsTitle} stands apart from competitors through its unique combination of persistent identity tracking across non-overlapping camera views and blind spots, a capability not found in Hikvision's proprietary hardware-dependent solution which excels at single-camera tracking but fails across non-overlapping views with time gaps. Unlike YOLORe-IDNet's research implementation which lacks continuous identity maintenance, our system preserves identity even when subjects disappear temporarily. JARVIS-MoCap requires controlled environments and specialized equipment, while our system works with existing camera infrastructure and adapts to varying lighting conditions through self-calibrating algorithms. Our multi-model fusion approach ensures reliable tracking even in campus blind spots where other systems lose track. The system's interactive 3D mapping provides security personnel with comprehensive spatial awareness that surpasses the limited single-view representations of competitors, making it uniquely suited for complex campus environments where maintaining continuous identity is critical for security operations.

\section{Literature Review}
\label{section:literature-review}

The development of multi-camera person tracking systems represents a significant advancement in computer vision and surveillance technology, addressing the fundamental challenge of maintaining consistent identification of individuals as they move through spaces monitored by multiple cameras. This literature review examines the current state of the art in multi-camera tracking, focusing on the three key technical challenges our system addresses: multi-view person detection, cross-camera re-identification and tracking, and unified spatial mapping.


Zhang et al. \cite{bytetrack} introduced ByteTrack, a simple yet effective association method that tracks by associating nearly all detection boxes rather than only high-confidence ones. This approach significantly reduces missed detections and trajectory fragmentation, achieving state-of-the-art performance on multiple tracking benchmarks. While ByteTrack excels in maintaining object identity despite occlusions and low-confidence detections, it lacks our system's cross-camera re-identification capabilities. Our system leverages ByteTrack's comprehensive detection approach while extending it with persistent identity preservation across multiple non-overlapping camera views.


He et al. \cite{fastreid} developed FastReID, a modular and extensible software system for general instance re-identification. Its architecture allows researchers to easily implement and test new ideas without rewriting the codebase, accelerating the transition from research to production. Unlike FastReID, which focuses primarily on the re-identification algorithm itself, our system integrates re-identification as part of a complete tracking solution with spatial awareness and environmental adaptation. Our implementation builds upon FastReID's powerful feature extraction techniques while adding context-aware identity maintenance specifically optimized for campus environments.


For 3D spatial mapping, Bredereke et al. \cite{modular3d} proposed a modular pipeline that calculates 3D trajectories of multiple objects using RGB cameras. Their approach is adaptable to various settings with multiple time-synchronized stationary cameras, demonstrating the feasibility of creating accurate spatial representations from multi-camera inputs. However, their system requires precise camera synchronization and calibration that limits practical deployment. Our system extends this approach with self-calibrating algorithms that can work with existing heterogeneous camera infrastructure while providing an interactive visualization interface absent in their implementation.


Woo et al. \cite{mtmmc} presented MTMMC, a comprehensive real-world dataset captured by 16 multi-modal cameras across campus and factory environments under varying conditions. This dataset serves as a valuable benchmark for multi-camera tracking systems and highlights the importance of handling diverse environmental conditions. While MTMMC provides excellent testing data, it doesn't offer a complete tracking solution. Our system utilizes insights from this dataset to develop adaptive algorithms that maintain tracking performance across the varying lighting and environmental conditions typical in campus settings.


Gautam et al. \cite{yoloreidnet} proposed YOLORe-IDNet, an efficient multi-camera system for person tracking that combines correlation filters and IOU constraints with a deep learning model for cross-camera person re-identification. Their system demonstrates robust performance in security applications, particularly in recovering identities after occlusion. However, YOLORe-IDNet lacks our system's unified spatial mapping and requires significant manual tuning for new environments. Our implementation builds upon their fusion of detection and re-identification while adding trajectory prediction models that maintain identity through blind spots not covered by cameras.

In conclusion, while existing systems have made significant progress in multi-camera tracking, there remains a gap in solutions that effectively integrate persistent identity tracking, environmental adaptation, and interactive spatial mapping for campus applications. Our \usevar{\srsTitle} addresses this gap by combining state-of-the-art computer vision techniques designed for practical deployment in real-world environments.